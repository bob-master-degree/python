# FactCheck — веб‑приложение для факт‑проверки текста

## 1. Цель и назначение
Приложение выделяет в произвольном тексте утверждения и оценивает их достоверность с помощью LLM. Пользователь вставляет текст, получает список утверждений с вердиктами и краткими обоснованиями — быстро и без перезагрузки страницы.

## 2. Описание проекта

Веб‑приложение предназначено для автоматической факт‑проверки текста с помощью LLM (модели OpenAI). Пользователь вставляет произвольный текст, после чего система выполняет следующие шаги:

- Текст разбивается на отдельные предложения.
- Для каждого предложения определяется, является ли оно фактическим утверждением (не вопросом, не мнением и не просьбой).
- Из всех предложений отбираются только утверждения.
- Для каждого утверждения формируется структурированный объект с полями:
    - `claim` — краткая суть утверждения (1–2 предложения, перефразировано).
    - `snippet` — точный фрагмент исходного текста, на который ссылается утверждение.
    - `verdict` — оценка достоверности (`true`, `likely_true`, `disputed`, `false`, `unverifiable`).
    - `reason` — краткое объяснение, почему дано такое заключение.
- Все объекты‑утверждения собираются в список и возвращаются в формате JSON.
- Интерфейс приложения разделён на две колонки: слева — исходный текст, справа — результаты анализа. Для обновления результатов используется HTMX: отправляется асинхронный POST‑запрос, сервер возвращает HTML‑фрагмент с анализом, который подменяет соответствующий блок без перезагрузки страницы.
- Для работы с OpenAI API требуется персональный API‑ключ пользователя. Ключ сохраняется только в сессионной cookie браузера, подписанной на сервере через SessionMiddleware. Ключ не хранится на сервере и не записывается в базу данных — только в зашифрованной cookie, доступной исключительно пользователю.

## 3. Используемые технологии

- **Backend**:
  - **Flask** — основной web‑фреймворк, реализует все HTTP‑эндпоинты, обработку запросов, интеграцию с шаблонами и middleware.
  - **Starlette SessionMiddleware** — обеспечивает работу с сессионными cookie: сериализация, подпись, восстановление состояния пользователя между запросами.
  - **CORS (Cross-Origin Resource Sharing)** — разрешает безопасные запросы с фронтенда, если он будет вынесен на отдельный домен.
- **Шаблоны**:
  - **Jinja2** — шаблонизатор для генерации HTML‑страниц и фрагментов, поддерживает наследование, макросы, фильтры.
- **LLM SDK**:
  - **openai** — официальный Python SDK для работы с OpenAI API (Chat Completions, модели GPT).
  - Используется модель **gpt-4o-mini** (можно заменить на другую при необходимости).
- **Frontend**:
  - **Tailwind CSS (CDN)** — утилитарный CSS‑фреймворк, подключается через CDN, кастомизация палитры через конфиг.
  - **HTMX** — библиотека для асинхронных запросов и динамического обновления DOM без перезагрузки страницы. Используются атрибуты: `hx-post`, `hx-target`, `hx-swap`, `htmx-indicator` для отправки форм, подмены блоков, отображения индикаторов загрузки.
- **Инфраструктура**:
  - **Docker** — контейнеризация приложения для удобного деплоя и изоляции зависимостей.
  - **docker-compose** — оркестрация сервисов (backend, nginx) в единой конфигурации.
  - **Nginx** — обратный прокси, раздаёт статику и проксирует API‑запросы к backend.

## 4. Архитектура и структура проекта

- `backend/main.py` — точка входа приложения:
  - Инициализация Flask‑приложения.
  - Подключение CORS‑middleware для поддержки кросс‑доменных запросов.
  - Подключение SessionMiddleware с секретом из переменной окружения (`APP_SESSION_SECRET`).
  - Настройка статики (раздача файлов из папки `static`).
  - Импорт и подключение всех роутов из `routes.py`.
- `backend/api/routes.py` — определяет все HTTP‑маршруты и логику рендеринга:
  - `GET /` — редирект на `/analyze` (стартовая страница).
  - `GET /settings` — страница для ввода и удаления OpenAI API‑ключа.
  - `POST /settings` — сохранение ключа в сессии пользователя.
  - `POST /settings/delete` — удаление ключа из сессии.
  - `GET /analyze` — форма для ввода текста и запуска анализа.
  - `POST /analyze` — обработка текста, вызов LLM, возврат HTML‑фрагмента с результатами или ошибкой.
- `backend/core/gpt_client.py` — взаимодействие с OpenAI API:
  - Формирует промпт для LLM.
  - Отправляет запрос к Chat Completions с нужными параметрами (температура, модель).
  - Парсит и валидирует JSON‑ответ, возвращает структуру для рендера.
- `backend/templates/*.html` — Jinja2‑шаблоны:
  - `base.html` — базовый шаблон с подключением Tailwind, HTMX, общими стилями.
  - `analyze.html` — страница анализатора (форма + контейнер для результатов).
  - `_analysis_result.html` — частичный шаблон для вывода результатов анализа.
  - `_toast_error.html` — шаблон для отображения ошибок.
  - `settings.html` — страница управления API‑ключом.

## 5. UX

1. Пользователь открывает страницу `/analyze`. В левой колонке — форма для ввода текста, в правой — блок для результатов.
2. После ввода текста и нажатия кнопки «Проверить факты» форма отправляется через HTMX (`hx-post="/analyze"`, `hx-target="#result"`), что инициирует асинхронный POST‑запрос без перезагрузки страницы.
3. На время запроса кнопка блокируется (`disabled`), отображается индикатор загрузки (`htmx-indicator`).
4. Сервер‑backend получает запрос, извлекает текст и проверяет наличие OpenAI API‑ключа в сессии пользователя (`request.session`).
   - Если ключ отсутствует — возвращается HTML‑фрагмент с ошибкой (toast), UI показывает уведомление.
5. Если ключ есть, backend вызывает функцию анализа (`analyze_text_claims`), которая формирует промпт и отправляет запрос к OpenAI API.
6. Ответ LLM парсится, валидируется (ожидается строгое соответствие JSON‑структуре).
   - Если ответ невалидный — возвращается пустой список фактов или сообщение об ошибке.
7. Результаты анализа (список утверждений с вердиктами и обоснованиями) рендерятся в частичный шаблон (`_analysis_result.html`), который HTMX подставляет в правую колонку UI.

## 6. UI

- **Tailwind CSS**:
  - Подключён через CDN в `base.html`, используется кастомная палитра (`primary`).
  - Все элементы интерфейса стилизованы через utility‑классы Tailwind.
- **HTMX**:
  - Формы отправляются асинхронно (`hx-post`), результат подменяет нужный контейнер (`hx-target`, `hx-swap`).
  - Индикатор загрузки (`htmx-indicator`) появляется на время запроса.
  - Кнопка отправки получает классы `disabled:*` (фон, курсор, pointer-events) для явного неактивного состояния во время обработки.
- **Вёрстка**:
  - Две колонки (Flex): слева — форма для текста, справа — результаты анализа.
  - Высота левой колонки не зависит от правой, обеспечивается адаптивность.
- **Ошибки и уведомления**:
  - Все ошибки (например, отсутствие ключа, сбой OpenAI) отображаются через toast‑уведомления (`_toast_error.html`), которые подставляются в UI без перезагрузки.

## 7. Prompt-design

- **Инструкция для LLM**:
  - Разбить входной текст на отдельные предложения.
  - Отфильтровать только фактические утверждения (исключить вопросы, мнения, просьбы).
  - Для каждого утверждения вернуть объект с полями:
    - `claim` — краткая суть утверждения (1–2 предложения, перефразировано).
    - `snippet` — точный фрагмент исходного текста, на который ссылается утверждение.
    - `verdict` — оценка достоверности (одно из: `true`, `likely_true`, `disputed`, `false`, `unverifiable`).
    - `reason` — краткое объяснение, почему дано такое заключение.
  - Вернуть строго JSON‑объект вида `{ "facts": [ ... ] }` (никаких пояснений вне JSON).
- **Параметры запроса**:
  - Температура: `0` (минимальная вариативность, максимальная предсказуемость формата).
  - Модель: `gpt-4o-mini`.
- **Обработка ошибок**:
  - Если ответ невалидный (не JSON или не соответствует схеме) — сервер возвращает пустой список фактов или ошибку для UI.

## 8. Сохранение и чтение OpenAI API Key

- **Где хранится ключ**: только в сессионной cookie пользователя, подписанной на сервере через SessionMiddleware.
- **Сценарии работы с ключом**:
  - **Сохранение**:
    - Пользователь открывает `/settings`, вводит свой OpenAI API‑ключ и отправляет форму (`POST /settings`).
    - Сервер сохраняет ключ в `request.session["openai_api_key"]`.
    - При формировании ответа SessionMiddleware сериализует всю сессию, подписывает её секретом (`APP_SESSION_SECRET`) и устанавливает cookie `factcheck_session` (HttpOnly).
  - **Чтение**:
    - На каждом запросе middleware читает cookie, проверяет подпись и восстанавливает `request.session`.
    - При анализе текста (`POST /analyze`) функция `analyze_text_claims` внутри `gpt_client.py` извлекает ключ из сессии.
    - Если ключ отсутствует — возвращается ошибка, UI показывает toast‑уведомление.
  - **Удаление**:
    - Пользователь отправляет `POST /settings/delete`.
    - Сервер удаляет ключ из сессии, cookie перезаписывается без ключа.

## 9. Запуск в Docker
```bash
docker compose -f docker/docker-compose.yml up --build
```
Рекомендуется передать `APP_SESSION_SECRET` через окружение/секреты оркестратора, чтобы все реплики использовали один и тот же секрет.

## 10. Переменные окружения
- `APP_SESSION_SECRET` — секрет подписи cookie‑сессий (обязателен, без значения сессии работать не будут).

## 11. Заключение
Проект демонстрирует компактную интеграцию LLM‑анализатора с удобным интерфейсом на HTMX и Tailwind. Хранение API‑ключа в подписанной сессии и простая архитектура позволяют развернуть приложение быстро и безопасно.
